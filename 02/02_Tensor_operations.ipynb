{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02.Tensor operations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhTm-wTJNJEJ",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning & Applied AI\n",
        "\n",
        "# Tutorial 2: Tensors operations\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "\n",
        "- Tensors operations: broadcasting, (not)-elementwise operations, tensors contraction, einsum\n",
        "\n",
        "Our info:\n",
        "\n",
        "- Luca Moschella (moschella@di.uniroma1.it)\n",
        "- Antonio Norelli (norelli@di.uniroma1.it)\n",
        "\n",
        "Course:\n",
        "\n",
        "- Website and notebooks will be available at [DLAI-s2-2020](https://erodola.github.io/DLAI-s2-2020/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dHLO4Z-T_yxB"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "You should familiarize with the [PyTorch Documentation](https://pytorch.org/docs/stable/) as it will greatly assist you.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pRePt-K1_yw9",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.__version__\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0B2Y47YJY97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Union\n",
        "\n",
        "# Utility print function\n",
        "def print_arr(*arr: Union[torch.Tensor, np.ndarray], prefix: str = \"\") -> None:\n",
        "    \"\"\" Pretty print tensors, together with their shape and type\n",
        "    \n",
        "    :param arr: one or more tensors\n",
        "    :param prefix: prefix to use when printing the tensors\n",
        "    \"\"\"\n",
        "    print(\n",
        "        \"\\n\\n\".join(\n",
        "            f\"{prefix}{str(x)} <shape: {x.shape}> <dtype: {x.dtype}>\" for x in arr\n",
        "        )\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bie5eT1Md_FW",
        "colab_type": "text"
      },
      "source": [
        "####Set torch and numpy random seeds for reproducibility\n",
        "If you are going to use a gpu, two further options must be set. (CuDNN is a library of CUDA for Deep Neural Networks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tGN_bJOcfd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(0)\n",
        "\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SUl8vYRv_yuG"
      },
      "source": [
        "### **Tensor operations**\n",
        "\n",
        "Functions that operate on tensors are often accessible in different ways, with the same meaning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bFFU6Xw7_yuA",
        "colab": {}
      },
      "source": [
        "t = torch.rand(3,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3TKgi2HL_yt_"
      },
      "source": [
        "Operators **overload**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a1377eee-67e8-49e3-bc74-8482f5bc4a84",
        "id": "HydKd9OK_yt7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "t + t"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.7645, 1.8300, 0.7657],\n",
              "        [1.9186, 0.7809, 1.2018],\n",
              "        [0.5131, 1.5873, 1.8815]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Amiof4a_yt6"
      },
      "source": [
        "Functions in the **``torch`` module**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d73484fe-f34f-49b0-c90d-135982c10bc9",
        "id": "7FKDL8ZG_yt2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "torch.add(t, t)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.7645, 1.8300, 0.7657],\n",
              "        [1.9186, 0.7809, 1.2018],\n",
              "        [0.5131, 1.5873, 1.8815]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zD99wCT8_yt0"
      },
      "source": [
        "Tensors **methods**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2e12ecba-3646-460d-88dd-0d0c3ef41069",
        "id": "_zkKPFHo_ytw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "t.add(t)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.7645, 1.8300, 0.7657],\n",
              "        [1.9186, 0.7809, 1.2018],\n",
              "        [0.5131, 1.5873, 1.8815]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aQ9ChdjH_ytv"
      },
      "source": [
        "#### **Basic operations and broadcasting**\n",
        "\n",
        "Basic mathematical operations $(+, -, *, /, **)$ are applied **elementwise or** do **broadcasting**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8c315e86-5d5a-4361-d1de-2811fce7a251",
        "id": "doNfhKA5_ytq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
        "y = torch.tensor([[5, 6], [7, 8]], dtype=torch.float64)\n",
        "\n",
        "print(x + y)  # elementwise sum\n",
        "print(x + 4.2)  # broadcasting"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.,  8.],\n",
            "        [10., 12.]], dtype=torch.float64)\n",
            "tensor([[5.2000, 6.2000],\n",
            "        [7.2000, 8.2000]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "829fa20b-3dce-4263-ae1b-4ee415fdac0b",
        "id": "GlZG4i_D_ytj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# other examples\n",
        "print(x * y - 5)\n",
        "print(x - y / y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  7.],\n",
            "        [16., 27.]], dtype=torch.float64)\n",
            "tensor([[0., 1.],\n",
            "        [2., 3.]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rNTtIs2NMKt",
        "colab_type": "text"
      },
      "source": [
        "Broadcasting is even more powerful..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XzPdOKXNLV-",
        "colab_type": "code",
        "outputId": "2e84adeb-15c5-4854-8474-74344cbaad97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "m = torch.arange(12).reshape(4, 3)\n",
        "v = torch.tensor([100, 0, 100])\n",
        "n = m + v\n",
        "print_arr(m, v, n)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([100,   0, 100]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[100,   1, 102],\n",
            "        [103,   4, 105],\n",
            "        [106,   7, 108],\n",
            "        [109,  10, 111]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYREKQAuQNYy",
        "colab_type": "code",
        "outputId": "dee288dc-f48b-436f-c65f-2e10371daeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "m = torch.arange(12).reshape(4, 3)\n",
        "u = torch.tensor([0, 10, 0, 10]).reshape(4,1)\n",
        "n = m + u\n",
        "print_arr(m, u, n)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 0],\n",
            "        [10],\n",
            "        [ 0],\n",
            "        [10]]) <shape: torch.Size([4, 1])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 0,  1,  2],\n",
            "        [13, 14, 15],\n",
            "        [ 6,  7,  8],\n",
            "        [19, 20, 21]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReNq-RtKg1Dy",
        "colab_type": "code",
        "outputId": "0026da1e-7ea2-47de-c61a-6efaeeebb8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "w = u + v\n",
        "print_arr(u, v, w)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0],\n",
            "        [10],\n",
            "        [ 0],\n",
            "        [10]]) <shape: torch.Size([4, 1])> <dtype: torch.int64>\n",
            "\n",
            "tensor([100,   0, 100]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[100,   0, 100],\n",
            "        [110,  10, 110],\n",
            "        [100,   0, 100],\n",
            "        [110,  10, 110]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYNysSHoQ6dX",
        "colab_type": "text"
      },
      "source": [
        "Master broadcasting is very useful to write **vectorized** code, i.e. code that avoids explicit python loops which are so slow. \n",
        "\n",
        "Instead, this approach takes advantage of the underlying C implementation of PyTorch and Numpy (on CPU) or CUDA implementation of Pytorch (on GPU).\n",
        "\n",
        "![broadcasting](https://jakevdp.github.io/PythonDataScienceHandbook/figures/02.05-broadcasting.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PqyMVYPL_yoC"
      },
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Given two vectors $X \\in R^n$ and $Y \\in R^m$ compute the differences between all possible pairs of numbers, and organize those differences in a matrix $Z \\in R^{n \\times m}$:\n",
        "> $$ z_{ij} = x_i - y_j $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WqFqMkksOBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gtF2--mB_yn1",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution (double click here to peek 👀)\n",
        "\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5])\n",
        "out = x[:, None] - y[None, :]\n",
        "\n",
        "print_arr(x, y, out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_L_GYRgsnLo",
        "colab_type": "text"
      },
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Given a  ${n \\times m}$ tensor and two indices $a \\in [0, n)$, $b \\in [0, m)$ and $p \\in [0, +\\inf)$,\n",
        "> create a new tensor $Y \\in R^{n \\times m}$ such that:\n",
        ">\n",
        "> $$ y_{ij} = d_{L_p}( (i,j), (a,b) ) \\text{ for each }  i \\in [0, n), j \\in  [0, m) $$\n",
        ">\n",
        "> That is, consider pairs of indices as points in $R^2$. Try different values of $p$ to see what happens.\n",
        ">\n",
        "> e.g. Using the $L_1$ distance given $(i,j) = (3, 5)$ and $(a,b) = (14, 20)$ we get:\n",
        "> $$ y_{3,5} = d_{L_1}( (3, 5), (14, 20) ) = |3 - 14| + |5 - 20| $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdW4Xf964XQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function\n",
        "import plotly.express as px\n",
        "\n",
        "def plot_row_images(images: Union[torch.Tensor, np.ndarray]) -> None:\n",
        "  \"\"\" Plots the images in a subplot with multiple rows.\n",
        "\n",
        "  Handles correctly grayscale images.\n",
        "\n",
        "  :param images: tensor with shape [number of images, width, height, <colors>]\n",
        "  \"\"\"\n",
        "  from plotly.subplots import make_subplots\n",
        "  import plotly.graph_objects as go\n",
        "  fig = make_subplots(rows=1, cols=images.shape[0] ,\n",
        "                      specs=[[{}] * images.shape[0]])\n",
        "  \n",
        "  # Convert grayscale image to something that go.Image likes\n",
        "  if images.dim() == 3:\n",
        "    images = torch.stack((images, images, images), dim= -1)\n",
        "  elif (images.dim() == 4 and images.shape[-1] == 1):\n",
        "    images = torch.cat((images, images, images), dim= -1)\n",
        "\n",
        "  assert images.shape[-1] == 3 or images.shape[-1] == 4\n",
        "    \n",
        "  for i in range(images.shape[0]):  \n",
        "    i_image = np.asarray(images[i, ...])\n",
        "\n",
        "    fig.add_trace( \n",
        "        go.Image(z = i_image, zmin=[0, 0, 0, 0], zmax=[1, 1, 1, 1]),\n",
        "        row=1, col=i + 1\n",
        "    )\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "# When using plotly pay attention that often it does not like PyTorch Tensors\n",
        "# ...and it does not give any error, just a empty plot."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApPt8XdAuK7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.zeros(300, 300)\n",
        "a = 150\n",
        "b = 150\n",
        "\n",
        "x[a, b] = 1  # Just to visualize the starting point\n",
        "plot_row_images(x[None, :])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mFwiTbVV7iho",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUDYSUxLuoAK",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution 1 (double click here to peek 👀)\n",
        "rows = torch.arange(x.shape[0])\n",
        "cols = torch.arange(x.shape[1])\n",
        "\n",
        "# Manual computation of L1\n",
        "y = (torch.abs(rows - a)[:, None] + torch.abs(cols - b)[None, :])\n",
        "px.imshow(y).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYF38G6dwAbT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution 2 (double click here to peek 👀)\n",
        "\n",
        "# Parametric computation of Lp\n",
        "p = 8\n",
        "y = ((torch.abs(rows - a ) ** p )[:, None] + \n",
        "     (torch.abs(cols - b) ** p)[None, :]) ** (1/p)\n",
        "px.imshow(y).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O530uju9a0h",
        "colab_type": "text"
      },
      "source": [
        "Solution 2 breaks with `p=10`. Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYZfn5gJ5kOM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Follow-up solution (double click here to peek 👀)\n",
        "\n",
        "# This works even with p=10. Why?\n",
        "p = 100\n",
        "y = ((torch.abs(rows.double() - a ) ** p )[:, None] + \n",
        "     (torch.abs(cols.double() - b) ** p)[None, :]) ** (1/p)\n",
        "px.imshow(y).show()\n",
        "\n",
        "# ->\n",
        "p = 10\n",
        "#print(torch.tensor(10, dtype=torch.int) ** p)\n",
        "#print(torch.tensor(10, dtype=torch.double) ** p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezcEIBPFSlFa"
      },
      "source": [
        "##### **Broadcasting, let's take a peek under the hood**\n",
        "\n",
        "In short: if a PyTorch operation supports broadcast, then **its Tensor arguments can be automatically expanded to be of equal sizes** (without making copies of the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CfTF3SiYSlFg"
      },
      "source": [
        "###### **Broadcastable tensors**\n",
        "\n",
        "Two tensors are \"broadcastable\" if:\n",
        "- Each tensor has at least one dimension\n",
        "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension **sizes** must either **be equal**, **one of them is 1**, or **one of them does not exist**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mdgd4qM5SlFh"
      },
      "source": [
        "###### **Broadcasting rules**\n",
        "\n",
        "Broadcasting two tensors together follows these rules:\n",
        "\n",
        "1. All input tensors have **1's prepended to their shapes**, to match the rank of the biggest tensor in input\n",
        "2. The size in each dimension of the **output shape** is the maximum of all the input sizes in that dimension\n",
        "3. An input can be used in the computation if its size in a particular **dimension either match** the output size in that dimension, **or has value exactly 1**\n",
        "4. If an input has a dimension size of 1 in its shape, the **first data entry in that dimension will be used for all calculations** along that dimension. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dcj42Be5SlFi"
      },
      "source": [
        "**In our example**:\n",
        "\n",
        "- `m` has shape (4,3)\n",
        "- `v` has shape (3,).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "94cfdeb8-50c8-49f7-e216-9d7b786389ef",
        "id": "lXQWIJtoSlFj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print_arr(m, v)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2],\n",
            "        [ 3,  4,  5],\n",
            "        [ 6,  7,  8],\n",
            "        [ 9, 10, 11]]) <shape: torch.Size([4, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([100,   0, 100]) <shape: torch.Size([3])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mZLeIamSSlFn"
      },
      "source": [
        "\n",
        "Following the Broadcasting logic, we can say the following is equivalent to what happened:\n",
        "\n",
        "- `v` has less dims than `m` so a dimension of `1` is **prepended** $\\to$ `v` is now `(1, 3)`.\n",
        "- Output shape will be `(max(1, 4), max(3, 3)) = (4, 3)`.\n",
        "- Dim 1 of `v` matches exactly (3); dim 0 is exactly 1, so we can use the first data entry in that dimension (i.e. the whole row 0 of `v`) for each time any row is accessed. This is effectively like converting `v` from `(1,3)` to `(4,3)` by replicating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xeoux-PvSlFp"
      },
      "source": [
        "\n",
        "For more on broadcasting, see the [documentation](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
        "\n",
        "Functions that support broadcasting are known as universal functions (i.e. ufuncs). For Numpy you can find the list of all universal functions in the [documentation](https://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vpE0yZGF_ytT"
      },
      "source": [
        "#### **Non-elementwise operations**\n",
        "\n",
        "\n",
        "PyTorch and NumPy provide many useful functions to perform computations on tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3f4d4455-423a-4881-db51-04756c8ef489",
        "id": "EI33i1Df_ytN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "print_arr(x)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]]) <shape: torch.Size([2, 2])> <dtype: torch.float32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "99f67601-d7c8-4248-c961-e75d81ee486c",
        "id": "6x4rhtfI_ytI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Sum up all the elements\n",
        "print_arr(torch.sum(x))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10.) <shape: torch.Size([])> <dtype: torch.float32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "319b8f9a-dfd1-4b7b-ae59-8c1459130ca0",
        "id": "OvndASPe_ytD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compute the mean of each column\n",
        "print_arr(torch.mean(x, dim=0))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2., 3.]) <shape: torch.Size([2])> <dtype: torch.float32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HUwZxvZP_ysy"
      },
      "source": [
        "> **REMEMBER!**\n",
        ">\n",
        "> In order to avoid confusion with the `dim` parameter, you can think of it as an index over the list returned by `tensor.shape`. The operation is performed iterating over that dimension.\n",
        "> \n",
        "> Visually: \n",
        "> \n",
        "><img src=\"https://qph.fs.quoracdn.net/main-qimg-30be20ab9458b5865b526d287b4fef9a\" width=\"500\" >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a6f3e267-bad8-4f8d-c237-f3151acbd896",
        "id": "4K-4z5pL_ys-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Compute the product of each row\n",
        "print_arr(torch.prod(x, dim=1))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2., 12.]) <shape: torch.Size([2])> <dtype: torch.float32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "94ce402b-091d-44d5-aa2e-c9eb8a98ccab",
        "id": "MRtgzF33_ys4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Max along the rows (i.e. max value in each column)\n",
        "values, indices = torch.max(x, dim=0)\n",
        "print_arr(values)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 4.]) <shape: torch.Size([2])> <dtype: torch.float32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7e0fae4b-8d33-43ba-f53e-6c01a93f55f1",
        "id": "Hd0zbRp0_ys0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Max along the columns (i.e. max value in each row)\n",
        "values, indices = torch.max(x, dim=1)\n",
        "print_arr(values)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2., 4.]) <shape: torch.Size([2])> <dtype: torch.float32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1xEs8jkkk4f",
        "colab_type": "text"
      },
      "source": [
        "###### **Dim parameter, let's take a peek under the hood**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0fa93f8b-3dca-4fe5-e79a-6c2459c02dca",
        "id": "DoDvtWHq_ysu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dim = 2\n",
        "\n",
        "a = torch.rand(2, 3, 4)\n",
        "out = a.sum(dim=dim)\n",
        "out"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.5308, 2.6237, 1.7376],\n",
              "        [1.6753, 1.3749, 1.9190]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b074505e-603a-4d5c-c226-b85cb48833b6",
        "id": "9-KbxoTK_ysq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# It is summing over the `dim` dimension, i.e.:\n",
        "a.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e36c18fc-de05-4c88-c1ce-959e8dcbbdc6",
        "id": "mZcG-q5R_ysm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The `dim` dimension has 4 elements\n",
        "a.shape[dim]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "636cd05c-84fa-4bdc-efad-577ab91b2590",
        "id": "zqT4jSkW_ysi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The dimension dim collapses, the output tensor will have shape:\n",
        "new_shape = a.shape[:dim] + a.shape[dim + 1:]\n",
        "new_shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35c8ee58-21bb-4b03-8a96-b283487f2e98",
        "id": "GHakpxbl_ysd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Explicitly compute the sum over dim\n",
        "out = torch.zeros(new_shape)                  \n",
        "for i in range(a.shape[dim]):\n",
        "  out += a.select(dim=dim, index=i)\n",
        "out\n",
        "\n",
        "# **DO NOT** use for loops in production"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.5308, 2.6237, 1.7376],\n",
              "        [1.6753, 1.3749, 1.9190]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQA3ngqoHsEt",
        "colab_type": "text"
      },
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Given a matrix $X \\in R^{k \\times k}$ compute the mean of the values along its diagonal. Perform this computation in at least two different ways, then check that the result is the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evQYg9-GH-Td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.rand(4, 4)\n",
        "print_arr(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x8-6wyGcB_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOEhv8X0ILC2",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution (double click here to peek 👀)\n",
        "a = torch.mean(x[torch.arange(x.shape[0]), torch.arange(x.shape[1])])\n",
        "b = torch.sum(torch.eye(x.shape[0]) * x) / x.shape[0]\n",
        "c = torch.trace(x) / x.shape[0]\n",
        "d = torch.mean(torch.diag(x))\n",
        "\n",
        "print(torch.equal(a, b) and torch.equal(a, c) and torch.equal(a, d))\n",
        "print_arr(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YePwf4ok_ysb"
      },
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Given a binary non-symmetric matrix $X \\in \\{0, 1\\}^{n, n}$, build the symmetric matrix $Y \\in \\{0, 1\\}^{n, n}$ defined as:\n",
        "> $$\n",
        "y_{ij} =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } x_{ij} = 1 \\\\\n",
        "1 & \\text{if } x_{ji} = 1 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$ \n",
        ">\n",
        "> *Hint*: search for `clamp` in the [docs](https://pytorch.org/docs/stable/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Dv-OmnV_ysU",
        "colab": {}
      },
      "source": [
        "x = torch.randint(0, 2, (5, 5))  # Non-symmetric matrix\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6HT6OSElDic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tM6Yd14JrAB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution (double click here to peek 👀)\n",
        "(x + x.t()).clamp(max=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cVC3YH8H_ysT"
      },
      "source": [
        "#### **Tensor contractions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QewYLJq-_yr5"
      },
      "source": [
        "##### **Matrix multiplication**\n",
        "\n",
        "Given $X \\in R^{n \\times d}$ and $Y \\in R^{d \\times v}$, their matrix multiplication $Z \\in R^{n \\times v}$ is defined as:\n",
        "\n",
        "$$ \\sum_{k=0}^{d} x_{ik} y_{kj} = z_{ij} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e8dba741-958d-4d47-ee7b-b7c49ac6fc7c",
        "id": "qu16frkd_yru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "y = torch.tensor([[1, 2], [2, 1]])\n",
        "print_arr(x, y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]]) <shape: torch.Size([3, 2])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[1, 2],\n",
            "        [2, 1]]) <shape: torch.Size([2, 2])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "60806290-b5f0-43da-b016-1ec78a96a359",
        "id": "PbVbXIzz_yrl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x @ y  # Operator overload"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5,  4],\n",
              "        [11, 10],\n",
              "        [17, 16]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4ecf450c-572a-4527-c403-14324e36a075",
        "id": "_FKQsMIG_yrh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "torch.mm(x, y)  # Explicit API"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5,  4],\n",
              "        [11, 10],\n",
              "        [17, 16]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "322406d0-4f75-423d-ba1f-809a5a3a3648",
        "id": "tTqIXFNU_yrZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "torch.einsum('ik, kj -> ij', (x, y))  # Einsum notation!\n",
        "\n",
        "# It summed up dimension labeled with the index `k`"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5,  4],\n",
              "        [11, 10],\n",
              "        [17, 16]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V-KPmcQg_ysS"
      },
      "source": [
        "##### **Dot product** \n",
        "Also known as Inner product. \n",
        "Given $x \\in R^k$ and $y \\in R^k$, the dot product $z \\in R$ is defined as:\n",
        "\n",
        "$$ \\sum_{i=0}^{k} x_i y_i = z $$\n",
        "\n",
        "Unlike MATLAB, ``*`` is the element wise multiplication, not the matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "aee04aa1-cb4c-48d1-fc12-e8d09e0eb4e2",
        "id": "6DxmdUoC_ysM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "print_arr(x, y)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([4, 5, 6]) <shape: torch.Size([3])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixf_iXF3KbOv",
        "colab_type": "code",
        "outputId": "b2726465-d2d6-4ecc-e3df-bdea1f3af889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We want to perform:\n",
        "(1 * 4) + (2 * 5) + (3 * 6)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f311efba-bbfc-4d05-c522-ae89eb65a113",
        "id": "c0jRMizc_ysG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.dot(x, y)  # PyTorch explicit API"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f58de829-fd77-4fb6-9748-6d2867c48c35",
        "id": "GrW2utM9_ysA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x @ y  # PyTorch operator overload"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7b369dae-b52a-4e0c-ccdd-02c5887330a8",
        "id": "xWV3hiAU_yr7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.einsum('i, i ->', (x, y))  # Einstein notation!\n",
        "\n",
        "# Multiply point-wise repeating indices in the input\n",
        "# Sum up along the indices that `do not` appear in the output"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3a9Yl0IC_yrW"
      },
      "source": [
        "##### **Batch matrix multiplication**\n",
        "\n",
        "Often we want to perform more operations together. Why?\n",
        "- Reduce the **overhead of uploading** each tensor to/from the GPU memory\n",
        "- **Better parallelization** of the computation\n",
        "\n",
        "Given two 3D tensors, each one containing ``b`` matrices,\n",
        "$X \\in R^{b \\times n \\times m}$\n",
        "and  \n",
        "$Y \\in R^{b \\times m \\times p}$, \n",
        "\n",
        "We want to multiply together each $i$-th couple of matrices, obtaining a tensor $Z \\in R^{b \\times n \\times p}$ defined as:\n",
        "\n",
        "$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{bkj} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "342af13c-10a7-43c5-c597-9c8170977725",
        "id": "wwUMo2Br_yrQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "x = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])\n",
        "y = torch.tensor([[[1, 2], [2, 1]], [[1, 2], [2, 1]]])\n",
        "print_arr(x, y)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1, 2],\n",
            "         [3, 4],\n",
            "         [5, 6]],\n",
            "\n",
            "        [[1, 2],\n",
            "         [3, 4],\n",
            "         [5, 6]]]) <shape: torch.Size([2, 3, 2])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[[1, 2],\n",
            "         [2, 1]],\n",
            "\n",
            "        [[1, 2],\n",
            "         [2, 1]]]) <shape: torch.Size([2, 2, 2])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "894d3993-01f4-410e-ca0d-cf9a2a5cbd38",
        "id": "FDW_9XKJ_yrG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "torch.bmm(x, y)  # **not** torch.mm"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]],\n",
              "\n",
              "        [[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bc37ce15-0fe8-4886-f63d-b9b533b6c102",
        "id": "zf7GaNmw_yrM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "x @ y"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]],\n",
              "\n",
              "        [[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ce808867-862b-4c16-b85e-593422d78fcf",
        "id": "wjjRPx-n_yrC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "torch.einsum('bik, bkj -> bij', (x, y)) # Einstein notation!"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]],\n",
              "\n",
              "        [[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC7y2peia8mp",
        "colab_type": "text"
      },
      "source": [
        "Can you feel the power of Einstein notation vibrating off of you?\n",
        "\n",
        "\n",
        "![Surfing einstein](https://roma.corriere.it/methode_image/2019/05/13/Roma/Foto%20Roma%20-%20Trattate/einstein2-kZfE-U3120295526975hVE-656x492@Corriere-Web-Roma.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S-aYQyHz_yrA"
      },
      "source": [
        "##### **Broadcast matrix multiplication**\n",
        "\n",
        "Given $b$ matrices with dimensions $n \\times m$ organized in one 3D tensor $X \\in R^{b \\times n \\times m}$\n",
        "and one 2D tensor $Y \\in R^{m \\times p}$, \n",
        "\n",
        "We want to multiply together each matrix $X_{i,:,:}$ with $Y$, obtaining a tensor $Z \\in R^{b \\times n \\times p}$ defined as:\n",
        "\n",
        "$$ z_{bij} = \\sum_{k=0}^m x_{bik} y_{kj} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ab14578c-3280-44d4-e793-e9b5a835e67c",
        "id": "awE5anPp_yq7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "x = torch.tensor([[[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4], [5, 6]]])\n",
        "y = torch.tensor([[1, 2], [2, 1]])\n",
        "print_arr(x, y)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1, 2],\n",
            "         [3, 4],\n",
            "         [5, 6]],\n",
            "\n",
            "        [[1, 2],\n",
            "         [3, 4],\n",
            "         [5, 6]]]) <shape: torch.Size([2, 3, 2])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[1, 2],\n",
            "        [2, 1]]) <shape: torch.Size([2, 2])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "002a843c-34ed-4f56-d19a-cc421215abdb",
        "id": "L7fxtm64_yq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "x @ y   # Operator overload: always use the last two dimensions"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]],\n",
              "\n",
              "        [[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "097d53c1-2db1-41a0-fc1e-2b2dd96308e7",
        "id": "fvZyCiYN_yqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "torch.matmul(x, y)  # Explicit PyTorch API: always use the last two dimensions"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]],\n",
              "\n",
              "        [[ 5,  4],\n",
              "         [11, 10],\n",
              "         [17, 16]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iod-0Z04_yqx"
      },
      "source": [
        "##### **EXERCISE**\n",
        ">\n",
        "> Use the einsum notation to compute the equivalent broadcast matrix multiplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LsgxdJnP_yqv"
      },
      "source": [
        "##### **Einsum notation**\n",
        "\n",
        "Einstein notation is a way to express complex operations on tensors\n",
        "\n",
        "- It is concise but enough expressive to do almost every operation you will need in building your neural networks, letting you think on the only thing that matters... **dimensions!**\n",
        "- You will not need to check your dimensions after an einsum operation, since the dimensions themselves are *defining* the tensor operation.\n",
        "-  You will not need to explicitly code intermediate operations such as reshaping, transposing and intermediate tensors\n",
        "- It is not library-specific, being avaiable in ``numpy``, ``pytorch`` and ``tensorflow`` with the same signature. So you do not need to remember the functions signature in all the frameworks.\n",
        "- Can sometimes be compiled to high-performing code (e.g. [Tensor Comprehensions](https://pytorch.org/blog/tensor-comprehensions/))\n",
        "\n",
        "Check [this blog post by Olexa Bilaniuk](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/) to take a peek under the hood of einsum and [this one by Tim Rocktäschel](https://rockt.github.io/2018/04/30/einsum) to look at even more examples than the ones that follows.\n",
        "\n",
        "Its formal behaviour is well described in the [Numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html).\n",
        "However, it is very intuitive and better explained through examples.\n",
        "\n",
        "![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-fmtstring.png?w=676)\n",
        "\n",
        "> *Historical note (taken from the Bilaniuk post)*\n",
        ">\n",
        "> Einstein had no part in the development of this notation. He merely popularized it, by expressing his entire theory of General Relativity in it. In a letter to [Tullio Levi-Civita](https://en.wikipedia.org/wiki/Tullio_Levi-Civita), co-developer alongside [Gregorio Ricci-Curbastro](https://en.wikipedia.org/wiki/Gregorio_Ricci-Curbastro) of Ricci calculus (of which this summation notation was only a part), Einstein wrote:\n",
        ">\n",
        "> \" *I admire the elegance of your method of computation; it must be nice to ride through these fields upon the horse of true mathematics while the like of us have to make our way laboriously on foot.* \""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UdyM0vLB_yqq",
        "colab": {}
      },
      "source": [
        "a = torch.arange(6).reshape(2, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ow4puwv4_yqp"
      },
      "source": [
        "###### **Matrix transpose**\n",
        "\n",
        "$$ B_{ji} = A_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ea53904e-09e6-494b-ce2c-f16e1342b9aa",
        "id": "LmE5nSrt_yqk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# The characters are indices along each dimension\n",
        "b = torch.einsum('ij -> ji', a)\n",
        "print_arr(a, b)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[0, 3],\n",
            "        [1, 4],\n",
            "        [2, 5]]) <shape: torch.Size([3, 2])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Z8j-iVH_yqi"
      },
      "source": [
        "###### **Sum**\n",
        "\n",
        "$$ b = \\sum_i \\sum_j A_{ij} := A_{ij} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f58f6ba4-8394-47e0-f2fb-aa1532f41b80",
        "id": "HFjp7cOb_yqd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Indices that do not appear in the output tensor are summed up\n",
        "b = torch.einsum('ij -> ', a)\n",
        "print_arr(a, b)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor(15) <shape: torch.Size([])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hks-z_NN_yqb"
      },
      "source": [
        "###### **Column sum**\n",
        "\n",
        "$$ b_j = \\sum_i A_{ij} := A_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "00e0829b-ed7b-44d0-b851-e49bdd9ceb44",
        "id": "MXbTLNtL_yqX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Indices that do not appear in the output tensor are summed up,\n",
        "# even if some other index appears\n",
        "b = torch.einsum('ij -> j', a)\n",
        "print_arr(a, b)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([3, 5, 7]) <shape: torch.Size([3])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1KQbgD-OEu4",
        "colab_type": "text"
      },
      "source": [
        "###### **EXERCISE**\n",
        ">\n",
        "> Which will be the shape and type of the following tensor $X \\in R^{100 \\times 200}$? Which values will it contain? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdBJvJt0OLsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = (torch.rand(100, 200) > 0.5).int()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDSB2iy0Mjf3",
        "colab_type": "text"
      },
      "source": [
        "###### **EXERCISE** \n",
        ">\n",
        "> Given a binary tensor $X \\in \\{0, 1\\}^{n \\times m}$ return a tensor $y \\in R^{n}$ that has in the $i$-th position the **number of ones** in the $i$-th row of $X$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sewLwDHNdQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display a binary matrix with plotly\n",
        "\n",
        "fig = px.imshow(x)\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-YO6eOmrUkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVQEYEThQn-I",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution (double click here to peek 👀) \n",
        "# Count the number of ones in each row\n",
        "row_ones = torch.einsum('ij -> i', x)\n",
        "\n",
        "row_ones2 = torch.sum(x, dim=-1)\n",
        "\n",
        "torch.equal(row_ones, row_ones2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUlzmmfTQ_p8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "px.imshow(row_ones[:, None]).show()\n",
        "print(f'Sum up the row counts: {row_ones.sum()}\\nSum directly all the ones in the matrix: {x.sum()}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JA5cWXv3_yqN"
      },
      "source": [
        "###### **Matrix-vector multiplication**\n",
        "\n",
        "$$ c_i = \\sum_k A_{ik}b_k := A_{ik}b_k $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0482099e-1756-43f9-b761-d32888fb823a",
        "id": "zYF4uNUC_yqJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Repeated indices in different input tensors indicate pointwise multiplication\n",
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(3)\n",
        "c = torch.einsum('ik, k -> i', [a, b])  # Multiply on k, then sum up on k\n",
        "print_arr(a, b, c)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([0, 1, 2]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([ 5, 14]) <shape: torch.Size([2])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qDsla37O_yqH"
      },
      "source": [
        "###### **Matrix-matrix multiplication**\n",
        "\n",
        "$$ C_{ij} = \\sum_k A_{ik}B_{kj} := A_{ik}B_{kj} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8khafOAKElM",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://obilaniu6266h16.files.wordpress.com/2016/02/einsum-matrixmul.png?w=676)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bb5da5e4-364e-4b67-eb90-90c1a49338fa",
        "id": "7mcYlmu5_yqD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(15).reshape(3, 5)\n",
        "c = torch.einsum('ik, kj -> ij', [a, b])\n",
        "print_arr(a, b, c)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 0,  1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8,  9],\n",
            "        [10, 11, 12, 13, 14]]) <shape: torch.Size([3, 5])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 25,  28,  31,  34,  37],\n",
            "        [ 70,  82,  94, 106, 118]]) <shape: torch.Size([2, 5])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wVAJP6Ma_yqC"
      },
      "source": [
        "###### **Dot product multiplication**\n",
        "\n",
        "$$ c = \\sum_i a_i b_i := a_i b_i $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cf0bd205-d4f1-4e7b-d920-cd13cb394845",
        "id": "2x-XwGOy_yp6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "a = torch.arange(3)\n",
        "b = torch.arange(3,6) \n",
        "c = torch.einsum('i,i->', (a, b))\n",
        "print_arr(a, b, c)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([3, 4, 5]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor(14) <shape: torch.Size([])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xhLZ4Gl__yp4"
      },
      "source": [
        "###### **Point-wise multiplication**\n",
        "Also known as hadamard product\n",
        "\n",
        "$$ C_{ij} = A_{ij} B_{ij} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "66c7ef93-a287-4130-9c34-d55c0fade8d7",
        "id": "QTUH61Ft_yp0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "a = torch.arange(6).reshape(2, 3)\n",
        "b = torch.arange(6,12).reshape(2, 3)\n",
        "c = torch.einsum('ij, ij -> ij', (a, b))\n",
        "print_arr(a, b, c)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 6,  7,  8],\n",
            "        [ 9, 10, 11]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 0,  7, 16],\n",
            "        [27, 40, 55]]) <shape: torch.Size([2, 3])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dZoiSCsn_ypz"
      },
      "source": [
        "###### **Outer product**\n",
        "\n",
        "$$ C_{ij} = a_i b_j $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eb38b9b3-e313-4367-ab6e-c07a96fdc0c9",
        "id": "k71BkJbf_ypu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "a = torch.arange(3)\n",
        "b = torch.arange(3,7)\n",
        "c = torch.einsum('i, j -> ij', (a, b))\n",
        "print_arr(a, b, c)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2]) <shape: torch.Size([3])> <dtype: torch.int64>\n",
            "\n",
            "tensor([3, 4, 5, 6]) <shape: torch.Size([4])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[ 0,  0,  0,  0],\n",
            "        [ 3,  4,  5,  6],\n",
            "        [ 6,  8, 10, 12]]) <shape: torch.Size([3, 4])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "df3bdabc-4a4d-4bd2-f3e1-9bfd7de0ae35",
        "id": "1MQYFeM2_ypo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Using the standard PyTorch API\n",
        "torch.ger(a, b)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  0],\n",
              "        [ 3,  4,  5,  6],\n",
              "        [ 6,  8, 10, 12]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HXQHwUet_yph"
      },
      "source": [
        "###### **Batch matrix multiplication**\n",
        "\n",
        "$$ c_{bij} = \\sum_k a_{bik} b_{bkj} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ad317063-9a8b-45a6-865d-d60b7b2450ac",
        "id": "6vpbY37H_ypb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "a = torch.randn(2,2,5)\n",
        "b = torch.randn(2,5,3)\n",
        "c = torch.einsum('ijk,ikl->ijl', [a, b])\n",
        "print_arr(a, b, c)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.8712, -0.2234,  1.7174,  0.3189,  1.1914],\n",
            "         [-0.8140, -0.7360, -0.8371, -0.9224,  1.8113]],\n",
            "\n",
            "        [[ 0.1606,  0.3672,  1.8446, -1.1845,  1.3835],\n",
            "         [-1.2024,  0.7078, -1.0759,  0.5357,  1.1754]]]) <shape: torch.Size([2, 2, 5])> <dtype: torch.float32>\n",
            "\n",
            "tensor([[[-1.9733e-01, -1.0546e+00,  1.2780e+00],\n",
            "         [ 1.4534e-01,  2.3105e-01,  8.6540e-03],\n",
            "         [-1.4229e-01,  1.9707e-01, -6.4172e-01],\n",
            "         [-2.2064e+00, -7.5080e-01,  2.8140e+00],\n",
            "         [ 3.5979e-01, -8.9808e-02,  2.8647e-02]],\n",
            "\n",
            "        [[ 6.4076e-01,  5.8325e-01,  1.0669e+00],\n",
            "         [-4.5015e-01, -6.7875e-01,  5.7432e-01],\n",
            "         [ 1.8775e-01,  1.7847e-01,  2.6491e-01],\n",
            "         [ 1.2732e+00, -1.3109e-03, -3.0360e-01],\n",
            "         [-9.8644e-01,  1.2330e-01,  3.4987e-01]]]) <shape: torch.Size([2, 5, 3])> <dtype: torch.float32>\n",
            "\n",
            "tensor([[[-0.3798,  0.8592, -1.2860],\n",
            "         [ 2.8596,  1.0533, -3.0532]],\n",
            "\n",
            "        [[-2.5890,  0.3457,  1.7146],\n",
            "         [-1.7685, -1.2295, -0.9128]]]) <shape: torch.Size([2, 2, 3])> <dtype: torch.float32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fm8KVyoyoNh",
        "colab_type": "text"
      },
      "source": [
        "###### **EXERCISE**\n",
        ">\n",
        "> - Matrix transpose with einsum ($Y = M^T$)\n",
        "> - Quadratic form with einsum  ($y = v^TMv$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKMXdwUdRN4U",
        "colab_type": "text"
      },
      "source": [
        "#### Singleton dimensions\n",
        "\n",
        " It is very common to **add or remove dimensions of size $1$** in a tensor.\n",
        "\n",
        " It is possible to perform these operations in different ways, feel free to use\n",
        " whatever is more comfortable to you.\n",
        "\n",
        " e.g. If we want to transform a rank-1 tensor into a rank-2 column tensor and the back to a rank-1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DsWB2_iRwTg",
        "colab_type": "code",
        "outputId": "527a1a8c-01a1-4ba3-bdf7-a8d6ce362b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define a rank-1 tensor\n",
        "x = torch.arange(6)\n",
        "print_arr(x)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFqraseqSAhO",
        "colab_type": "text"
      },
      "source": [
        "Transform **`x` into a column tensor** in tree different ways.\n",
        "\n",
        "Remember that the shape of a column tensor is in the form: `(rows, 1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-BC2G0TSSfO",
        "colab_type": "code",
        "outputId": "f2f8d7d4-a9e2-4a34-da8f-e9ccf54d0569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Use the `reshape` or `view` functions\n",
        "\n",
        "y1 = x.reshape(-1, 1)\n",
        "y2 = x.view(-1, 1)\n",
        "\n",
        "print_arr(y1, y2)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMOA51cBPp1i",
        "colab_type": "code",
        "outputId": "f72427ed-5f75-402a-91c7-d1652c7cc6d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Use the specific `unsqueeze` function to unsqueeze a dimension\n",
        "\n",
        "y3 = x.unsqueeze(dim=-1)\n",
        "y4 = x.unsqueeze(dim=1)\n",
        "\n",
        "print_arr(y3, y4)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n",
            "\n",
            "tensor([[0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXkhm2nVN8KZ",
        "colab_type": "code",
        "outputId": "9557eb3b-dfa7-437d-9f0e-96a42e1fecf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Explicitly index a non-exixtent dimension with `None`\n",
        "\n",
        "y5 = x[:, None]\n",
        "\n",
        "print_arr(y5)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0],\n",
            "        [1],\n",
            "        [2],\n",
            "        [3],\n",
            "        [4],\n",
            "        [5]]) <shape: torch.Size([6, 1])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XBTG4CNTcLf",
        "colab_type": "code",
        "outputId": "fdff2b44-e978-423c-9208-e35266a5e999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# To go back into a rank-1 tensor\n",
        "\n",
        "x1 = y1.reshape(-1)\n",
        "x2 = y2.view(-1)          # Explicity enforce to get a view of the tensors, without copying data\n",
        "x3 = y3.squeeze(dim=-1)\n",
        "x4 = y4.squeeze(dim=1)\n",
        "x5 = y5[:, 0]             # Manually collapse the dimension with an integer indexing\n",
        "\n",
        "print_arr(x1, x2, x3, x4, x5)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n",
            "\n",
            "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n",
            "\n",
            "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n",
            "\n",
            "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n",
            "\n",
            "tensor([0, 1, 2, 3, 4, 5]) <shape: torch.Size([6])> <dtype: torch.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMVrjaXWySWR",
        "colab_type": "text"
      },
      "source": [
        "> **NOTE**\n",
        ">\n",
        "> indexing with `...` means  **keeps all the other dimension the same**.\n",
        "> Keep in mind that `...` is just a Python singleton object (just as `None`). \n",
        "> Its type is Ellipsis:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNKIfvMTsipK",
        "colab_type": "code",
        "outputId": "9728f34d-4343-4b01-ea68-769ebbd1f1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "..."
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BErfK7zVFhAV",
        "colab_type": "code",
        "outputId": "55c64aca-f3d2-4a81-d94b-51f137bed128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x = torch.rand(3,3,3)\n",
        "x[:, :, 0]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3336, 0.2846, 0.3139],\n",
              "        [0.1568, 0.1054, 0.9302],\n",
              "        [0.8460, 0.0850, 0.3908]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo3ymGK7FhZS",
        "colab_type": "code",
        "outputId": "c3b2d05f-22ba-4b2f-f064-72c7690a9994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x[..., 0]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3336, 0.2846, 0.3139],\n",
              "        [0.1568, 0.1054, 0.9302],\n",
              "        [0.8460, 0.0850, 0.3908]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKyFGa7B6jXb",
        "colab_type": "text"
      },
      "source": [
        "### Tensor types\n",
        "Pay attention to the tensor types!\n",
        "Several methods are available to convert tensors to different types:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHIa9x_X6tnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.rand(3, 3) + 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5swdKvAa6w81",
        "colab_type": "code",
        "outputId": "097c0411-e6a8-43d8-f8c6-39e0d6e8d215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a.int()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0],\n",
              "        [1, 1, 1],\n",
              "        [1, 0, 1]], dtype=torch.int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQfh53W26x6R",
        "colab_type": "code",
        "outputId": "38d6ef8b-1b34-403d-d660-215156ed6dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a.long()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0],\n",
              "        [1, 1, 1],\n",
              "        [1, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22VBQVrG64Qq",
        "colab_type": "code",
        "outputId": "e91915e8-be19-4bd2-fd65-d18559c1d5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a.float()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3712, 0.6330, 0.9137],\n",
              "        [1.1044, 1.2581, 1.4037],\n",
              "        [1.4555, 0.6035, 1.1258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSdMzMOb6yk0",
        "colab_type": "code",
        "outputId": "f5287683-5be2-4aad-e3c1-db3514787696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a.double()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3712, 0.6330, 0.9137],\n",
              "        [1.1044, 1.2581, 1.4037],\n",
              "        [1.4555, 0.6035, 1.1258]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXKThOrW6zQF",
        "colab_type": "code",
        "outputId": "3b4ff03b-d7ce-4968-cd2c-2ab160c03f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a.bool()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLuyxzT_60Bw",
        "colab_type": "code",
        "outputId": "64775f01-cd45-44e6-b523-cc280049b3ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a.to(torch.double)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3712, 0.6330, 0.9137],\n",
              "        [1.1044, 1.2581, 1.4037],\n",
              "        [1.4555, 0.6035, 1.1258]], dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJDsriTC64-e",
        "colab_type": "code",
        "outputId": "c41ae63a-6417-4437-a900-4f9ebef02c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a.to(torch.uint8)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0],\n",
              "        [1, 1, 1],\n",
              "        [1, 0, 1]], dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YRWAuTW7Ybn",
        "colab_type": "code",
        "outputId": "7ef8e82a-4b67-4ebf-efdd-635dc70785c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a.bool().int()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1],\n",
              "        [1, 1, 1],\n",
              "        [1, 1, 1]], dtype=torch.int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uu0xWOBV_yny"
      },
      "source": [
        "---\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g7yW6kZ7_ynq"
      },
      "source": [
        "Do not try to memorize all the PyTorch API: \n",
        "> Learn to understand what operation should already exist and search for it, when you need it.\n",
        "\n",
        "Google, StackOverflow and the documentation are your friends!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lafVikgz_ypX"
      },
      "source": [
        "### Final exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPeJMqEACs8z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### **EXERCISE 1**\n",
        ">\n",
        "> You are given $b$ images with dimensions $w \\times h$. Each pixel in each image has an `(r, g, b)` $c$ channel. These images are organized in a tensor $X \\in R^{w \\times b \\times c \\times h}$.\n",
        ">\n",
        "> You want to apply a linear trasformation to the color channel of each single image. In particular, you want to convert each image into a grey scale image.\n",
        "> Afterthat, transpose the images.\n",
        ">\n",
        "> The linear traformation that converts from `(r, g, b)` to grey scale is simply a linear combination of `r`, `g` and `b`. It can be encoded in the following 1-rank tensor $y \\in R^3$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BRybv63oysl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = torch.tensor([0.2989, 0.5870, 0.1140], dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON5HBjD2oyLi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> You want to obtain a tensor $Z \\in R^{b \\times w \\times h \\times 3}$.\n",
        "> \n",
        "> Write the PyTorch code that performs this operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If137gBApDcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the input tensor for the exercise\n",
        "\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "image1 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Earth_Eastern_Hemisphere.jpg/260px-Earth_Eastern_Hemisphere.jpg')\n",
        "image1 = torch.from_numpy(resize(image1, (300, 301), anti_aliasing=True)).float()  # Covert  to float type\n",
        "image1 = image1[..., :3]  # remove alpha channel\n",
        "\n",
        "image2 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/b/b4/The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg/628px-The_Sun_by_the_Atmospheric_Imaging_Assembly_of_NASA%27s_Solar_Dynamics_Observatory_-_20100819.jpg')\n",
        "image2 = torch.from_numpy(resize(image2, (300, 301), anti_aliasing=True)).float()\n",
        "image2 = image2[..., :3]  # remove alpha channel\n",
        "\n",
        "image3 = io.imread('https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Wikipedia-logo-v2.svg/1920px-Wikipedia-logo-v2.svg.png')\n",
        "image3 = torch.from_numpy(resize(image3, (300, 301), anti_aliasing=True)).float()\n",
        "image3 = image3[..., :3]  # remove alpha channel\n",
        "\n",
        "source_images = torch.stack((image1, image2, image3), dim=0)\n",
        "images = torch.einsum('bwhc -> wbch', source_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGgBC-dkqwVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot source images\n",
        "plot_row_images(source_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flm4DsjMww90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwGXAyrVryTz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution (double click here to peek 👀) \n",
        "\n",
        "# Grey-fy all images together, using the `images` tensor\n",
        "gray_images = torch.einsum('wbch, c -> bwh', (images, y))\n",
        "\n",
        "# What if you want to transpose the images?\n",
        "gray_images_tr = torch.einsum('wbch, c -> bhw', (images, y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEWp-NDzsB4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the gray images\n",
        "plot_row_images(gray_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga_3wk7BPmp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the gray transposed images\n",
        "plot_row_images(gray_images_tr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1tHmuIBM9ka",
        "colab_type": "text"
      },
      "source": [
        "#### **EXERCISE 2**\n",
        ">\n",
        ">  You are given $b$ images with dimensions $w \\times h$. Each pixel in each image has an `(r, g, b)` $c$ channel. These images are organized in a tensor $X \\in R^{w \\times b \\times c \\times h}$, i.e. the same tensor as in the previous exercise.\n",
        ">\n",
        "> You want to swap the `red` color with the `blue` color, and decrese the intensity of the `green` by half\n",
        ">\n",
        "> Perform the transormation on all the images together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lppGxeMSO0c8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POWStC54w2ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE2xc7i_ORzg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution (double click here to peek 👀) \n",
        "\n",
        "# Define the linear tranformation to swap the blue and red colors\n",
        "S = torch.tensor([[ 0, 0, 1],\n",
        "                  [ 0, .5, 0],\n",
        "                  [ 1, 0, 0]], dtype=torch.float)\n",
        "\n",
        "# Apply the linear transformation to the color channel!\n",
        "rb_images = torch.einsum('wbch, dc -> bwhd', (images, S))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zdfYOMBOxjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_row_images(rb_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF6FdNroYMqn",
        "colab_type": "text"
      },
      "source": [
        "#### **EXERCISE 3**\n",
        ">\n",
        "> Given $k$ points organized in a tensor $X \\in R^{k \\times 2}$ apply a reflection along the $y$ axis as a linear transformation.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_d9KnJ5Y3e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define some points in R^2\n",
        "x = torch.arange(100, dtype=torch.float)\n",
        "y = x ** 2\n",
        "\n",
        "# Define some points in R^2\n",
        "data = torch.stack((x, y), dim=0).t()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0_JB4C1Y2vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "px.scatter(x = data[:, 0].numpy(), y = data[:, 1].numpy())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1Xt5XEkDYvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWCEwmIfaebY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution (double click here to peek 👀) \n",
        "\n",
        "# Define a matrix that encodes a linear transformation\n",
        "S = torch.tensor([[-1, 0],\n",
        "                  [ 0, 1]], dtype=torch.float)\n",
        "\n",
        "# Apply the linear transformation: the order is important\n",
        "new_data = torch.einsum('nk, dk -> nd', (data, S))\n",
        "\n",
        "# The linear transformation correclty maps the basis vectors!\n",
        "S @ torch.tensor([[0],\n",
        "                  [1]], dtype=torch.float)\n",
        "S @ torch.tensor([[1],\n",
        "                  [0]], dtype=torch.float)\n",
        "\n",
        "# Check if at least the shape is correct\n",
        "new_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H7VEJFXbaMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the new points\n",
        "px.scatter(x = new_data[:, 0].numpy(), y = new_data[:, 1].numpy())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLCQReiBPLqd",
        "colab_type": "text"
      },
      "source": [
        "#### **EXERCISE 4**\n",
        ">\n",
        ">  You are given $b$ images with dimensions $w \\times h$. Each pixel in each image has an `(r, g, b)` $c$ channel. These images are organized in a tensor $X \\in R^{w \\times b \\times c \\times h}$, i.e. the same tensor as exercise 1 and 2.\n",
        ">\n",
        "> You want to convert each image into a 3D point cloud, where the `(x, y)`  coordinates are the indices of the pixels, and the `z` coordinate is the $L_2$ norm of the color of each pixel, multiplied by $10$\n",
        ">\n",
        "> *Hint*: you may need some other PyTorch function, search the docs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxLy9MHNC26V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ✏️ your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk-O_iHgSlbX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Solution (double click here to peek 👀) \n",
        "\n",
        "# Just normalize the tensor into the common form [batch, width, height, colors]\n",
        "imgs = torch.einsum('wbch -> bwhc', images)\n",
        "imgs.shape\n",
        "\n",
        "# The x, y coordinate of the point cloud are all the possible pairs of indices (i, j)\n",
        "row_indices = torch.arange(imgs.shape[1], dtype=torch.float)\n",
        "col_indices = torch.arange(imgs.shape[2], dtype=torch.float)\n",
        "xy = torch.cartesian_prod(row_indices , col_indices)\n",
        "\n",
        "# Compute the L2 norm for each pixel in each image\n",
        "depth = imgs.norm(p=2, dim = -1)\n",
        "# depth = torch.einsum('bwhc, bwhc -> bwh', (imgs, imgs)) ** (1/2)\n",
        "\n",
        "# For every pair (i, j), retrieve the L2 norm of that pixel\n",
        "z = depth[:, xy[:, 0].long(), xy[:, 1].long()] * 10\n",
        "\n",
        "# Adjust the dimensions, repeat and concatenate accordingly\n",
        "xy = xy[None, ...]\n",
        "xy = xy.repeat(imgs.shape[0], 1, 1)\n",
        "z = z[..., None] \n",
        "clouds = torch.cat((xy, z), dim= 2)\n",
        "\n",
        "clouds.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1S0OYtiZgT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Union\n",
        "\n",
        "def plot_3d_point_cloud(cloud: Union[torch.Tensor, np.ndarray]) -> None:\n",
        "  \"\"\" Plot a single 3D point cloud\n",
        "\n",
        "  :param cloud: tensor with shape [number of points, coordinates]\n",
        "  \"\"\"\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(np.asarray(cloud), columns=['x', 'y', 'z'])\n",
        "  fig = px.scatter_3d(df, x=df.x, y=df.y, z=df.z, color=df.z, opacity=1, range_z=[0, 30])\n",
        "  fig.update_layout({'scene_aspectmode': 'data', 'scene_camera':  dict(\n",
        "          up=dict(x=0., y=0., z=0.),\n",
        "          eye=dict(x=0., y=0., z=3.)\n",
        "      )})\n",
        "  fig.update_traces(marker=dict(size=2,),\n",
        "                    selector=dict(mode='markers'))\n",
        "  _ = fig.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlKzABIKm9UZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_3d_point_cloud(clouds[0, ...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Jm-zYSbQYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_3d_point_cloud(clouds[1, ...])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH0CITRgZnyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_3d_point_cloud(clouds[2, ...])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
